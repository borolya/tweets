{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kYnhrup_YgdZ",
    "outputId": "abff869e-9b6d-43cf-82cc-7f5ef8580670"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "from scipy.sparse import vstack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OmB74k_reZIc"
   },
   "source": [
    "## Tweets overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okGzpbRUew00",
    "outputId": "13dd1f69-74ad-45c9-d667-a5b15e7ff096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t ***POSIRIVE*** \t\n",
      "An inspiration in all aspects: Fashion\n",
      " fitness\n",
      " beauty and personality. :)KISSES TheFashionIcon\n",
      "Apka Apna Awam Ka Channel Frankline Tv Aam Admi Production Please Visit Or Likes  Share :)Fb Page :...\n",
      "Beautiful album from  the greatest unsung guitar genius of our time - and I've met the great backstage\n",
      "Good luck to Rich riding for great project in this Sunday. Can you donate?\n",
      "Omg he... kissed... him crying with joy\n",
      "happy anniv ming and papi!!!!! love love happy\n",
      "thanks happy\n",
      "C'mon Tweeps\n",
      " Join  vote for the singer! Do spread the word. :D\n",
      "Thanks for the great review! smile\n",
      "Yay another art raffle! Everything you need to know is in the picture :D\n",
      "Hello I hope you visit Luxor its amazing city in Egypt pleas check\n",
      "We got a Vive tracker in the office and our intern\n",
      " went to work.Don't get too excited\n",
      " this isn't\n",
      "Take a look at favourites.io You can do this and more happy\n",
      "Go back to school for music! I think I will in time happy\n",
      "Sixth spot not applicable Team! Higher pa! :)KISSES TheFashionIcon\n",
      "thanks for being top influencers in my community last week happy\n",
      "Share the love: thanks for being top new followers this week happy  Want this?\n",
      "retweet this tweet if you want your to be included in the nhc scrapbook!! i'll be making something out of all of the usern\n",
      "Happy 420 happy  pass it to your followers!\n",
      "Who wants some good vibes? Watch this on YouTube or TV100 application now for FREE happy\n",
      "Great to work with you too such a lovely place! Thank you for havin\n",
      "Nothing is ever as it seems. What looks bad today\n",
      " will be a blessing tomorrow happy  Keep your faith stirred up each day!\n",
      "Mmmmmmmm love naughty wives smile\n",
      "You deserve a vacation happy\n",
      "Never known how hard it is until last to  days how to resist temptation to enter politics debates on social media happy  stood firm so far\n",
      "\n",
      " Describe:                                                         0\n",
      "count                                                1183\n",
      "unique                                               1001\n",
      "top     Thanks for the recent follow Happy to connect ...\n",
      "freq                                                   34\n",
      "\n",
      " Len: 1186\n",
      "\n",
      " Count: 1183\n",
      "\n",
      " \t ***NEGATIVE*** \t\n",
      "How unhappy  some dogs like it though\n",
      "talking to my over driver about where I'm goinghe said he'd love to go to New York too but since Trump it's probably not\n",
      "Does anybody know if the Rand's likely to fall against the dollar? I got some money  I need to change into R but it keeps getting stronger unhappy \n",
      "I miss going to gigs in Liverpool unhappy \n",
      "There isnt a new Riverdale tonight ? unhappy \n",
      "it's that A*dy guy from pop Asia and then the translator so they'll probs go with them around Aus unhappy \n",
      "Who's that chair you're sitting in? Is this how I find out. Everyone knows now. You've shamed me in pu\n",
      "don't like how jittery caffeine makes me sad \n",
      "My area's not on the list unhappy  think I'll go LibDems anyway\n",
      "I want fun plans this weekend unhappy \n",
      "When can you notice me.  unhappy  what?  \n",
      "Ahhhhh! You recognized LOGAN!!! Cinemax shows have a BAD track record for getting cancelled unhappy \n",
      "Errr dude.... They're gone unhappy  Asked other league memeber to check  the guys are go \n",
      "Not you again sad  \n",
      "Why would Harvey be going to prison? unhappy \n",
      "Missing in crying  Seaside area. \n",
      "Becoz if we will depend on your promoting its waste of hardwork to all team who \n",
      "I thought you'll save me crying \n",
      "major waffle cravings right now sad \n",
      "cant speak japanese ::(\n",
      "how can people do stuff like this unhappy  \n",
      "please just stop confining animals in zoos unhappy  \n",
      "Feel like i shoyould be telling you to get the fuck out social media byout also feel really mean because unhappy  silence  love yoyou hope yoyoure okay \n",
      "i miss you huhu so busy unhappy \n",
      "it was extended family. 12 ppl.ahh wanted to show a Oh My Girl being dorky while playing a game but it got deleted unhappy \n",
      "Don't do that unhappy \n",
      "Jamie can you please reset the CGa grandfinal server... no administrator are responding unhappy \n",
      "noOoooooo YOU GONNA MISS THE BUFFET unhappy  TAKE CARE AIN!!!!!!!!!!!!!!!!! \n",
      "I wish i could vote for you unhappy \n",
      "instant message so jealous okay unhappy  but never mind haha bruno can wait finals first\n",
      "when i'm enlisting can please turn up like this unhappy  \n",
      "\n",
      " Describe:                                                         0\n",
      "count                                                1116\n",
      "unique                                                981\n",
      "top     this is damn.... theres people who sincerely w...\n",
      "freq                                                   17\n",
      "\n",
      " Len: 1117\n",
      "\n",
      " Count: 1116\n",
      "\n",
      " \t ***NEUTRAL*** \t\n",
      "Pak PM survives removal scare\n",
      " but court orders further probe into corruption charge. \n",
      "Supreme Court quashes criminal complaint against cricketer for allegedly depicting himself as on magazine cover.\n",
      "Art of Living's fights back over Yamuna floodplain damage\n",
      " livid. \n",
      "FCRA slap on NGO for lobbying...But was it doing so as part of govt campaign? \n",
      "Why doctors\n",
      " pharma companies are opposing names on \n",
      "Why a bicycle and not a CM asked. His officer learnt ground reality -- and  a dip in a river. \n",
      "It's 2017. making law to ban And MHA is sitting on draft. \n",
      "Rivals govts unite to act against sex-determination tests. \n",
      "Haryana peasants demand justice for right to cattle trade. \n",
      "Why schools in Calcutta (and elsewhere) are stunned by imposition plan. \n",
      "Why renamed places in \n",
      "Oooh! the shame\n",
      " the trauma... of driving without the red\n",
      " flashing light. (And paying for parking). \n",
      "Now have to learn to live without that flashing red light. \n",
      "BJP leaders in the dock\n",
      " lose their light\n",
      " naming game\n",
      " and more. Also in epaper. \n",
      "UK Parliament votes by 522 to 13 in favour of early national \n",
      "April 19 1906: Nobel Laureate Pierre Curie was run over by a horse-drawn wagon near the Pont Neuf\n",
      " Paris\n",
      " and killed \n",
      "India to end culture of sirens on cars. \n",
      "From May 1\n",
      " only on vehicles: Gadkari.\n",
      "No from now on cars of PM\n",
      " ministers: transport minister Nitin Gadkari after Cabinet meet. \n",
      "\n",
      " Describe:                                   0\n",
      "count                          1569\n",
      "unique                         1478\n",
      "top      and more. Also in epaper. \n",
      "freq                             52\n",
      "\n",
      " Len: 1570\n",
      "\n",
      " Count: 1569\n"
     ]
    }
   ],
   "source": [
    "def print_overview(path):\n",
    "    df = pd.read_csv(path, header = None)\n",
    "    df = df.transpose()\n",
    "    for i in df.loc[:30, 0]:\n",
    "        print(i)\n",
    "    print('\\n Describe:', df.describe())\n",
    "    print('\\n Len:', len(df.index))\n",
    "    print('\\n Count:', df.count()[0])\n",
    "      \n",
    "print('\\t ***POSIRIVE*** \\t')\n",
    "print_overview('data/processedPositive.csv')\n",
    "print('\\n \\t ***NEGATIVE*** \\t') \n",
    "print_overview('data/processedNegative.csv')\n",
    "print('\\n \\t ***NEUTRAL*** \\t')\n",
    "print_overview('data/processedNeutral.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YrCj58Z5YzK-"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "preprocessing\n",
    "- remove nan\n",
    "- concat rows to tweet\n",
    "- remove duplication\n",
    "\n",
    "Input: csv filename\n",
    "Ountut: processing df\n",
    "\n",
    "'''\n",
    "\n",
    "def preprocessing(name_csv):\n",
    "    df = pd.read_csv(name_csv, header = None)\n",
    "    df.set_index(pd.Index(['tweet']), inplace = True)\n",
    "    df = df.transpose()\n",
    "    df.dropna()\n",
    "    df['tweet'] = df['tweet'].apply(str)\n",
    "\n",
    "    if len(df.index) < 1:\n",
    "        return None\n",
    "    return_df = pd.DataFrame()\n",
    "    tweet = df.iloc[0, 0]\n",
    "    for row in df.iloc[1:, 0]:\n",
    "        if len(row) < 1:\n",
    "            continue\n",
    "        if row[0].isspace():\n",
    "            tweet = (tweet.strip() + ' ' + row.strip()).strip()\n",
    "        else:\n",
    "            return_df = pd.concat([return_df, pd.DataFrame([tweet])], axis = 0)\n",
    "            tweet = row\n",
    "    return_df.columns = ['tweet']\n",
    "    return_df.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    return(return_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gTBj-5szY4Md"
   },
   "outputs": [],
   "source": [
    "def remove_nan_dupl(df, name):\n",
    "    nan_value = float(\"NaN\")\n",
    "    df_tweet = df[name].replace(\"\", nan_value)\n",
    "    df_tweet.dropna(inplace = True)\n",
    "    df_tweet.drop_duplicates(inplace=True)\n",
    "    df = df.loc[df_tweet.index]\n",
    "    df.index = list(range(len(df)))\n",
    "    return df\n",
    "\n",
    "def concatinate_all_tweets(pos, neg, neu):\n",
    "    pos['sense'] = 1\n",
    "    neg['sense'] = -1\n",
    "    neu['sense'] = 0\n",
    "    df = pd.concat([pos, neg, neu], axis = 0, ignore_index = True)\n",
    "    return(df)\n",
    "\n",
    "def to_lower(df):\n",
    "    df['tweet'] = df['tweet'].str.lower()\n",
    "    return(df)\n",
    "\n",
    "def remove_punctuation(df):\n",
    "    df['tweet'] = df['tweet'].str.replace('[^a-z_\\s]','', regex=True)\n",
    "    return(df)\n",
    "\n",
    "def delete_stopwords(text):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "n1_v352xY6S1"
   },
   "outputs": [],
   "source": [
    "with open('data/Emoticon_Dict.p', 'rb') as fp:\n",
    "    Emoticon_Dict = pickle.load(fp)\n",
    "\n",
    "def convert_emoticons_to_word(text):\n",
    "    for k, v in Emoticon_Dict.items():\n",
    "        emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in Emoticon_Dict) + u')')\n",
    "        repl = ' ' + v.replace(' ', '_') + ' ';\n",
    "        text = emoticon_pattern.sub(repl, text)\n",
    "    return text\n",
    "\n",
    "def check_most_frequent_words(df, count_words):\n",
    "    cnt = Counter()\n",
    "    for text in df[\"text_stop\"].values:\n",
    "        for word in text.split():\n",
    "            cnt[word] += 1\n",
    "    return(cnt.most_common(count_words))\n",
    "\n",
    "def freqwords(text):\n",
    "    cnt = check_most_frequent_words(df, 10)\n",
    "    freq = set([w for (w, wc) in cnt])\n",
    "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
    "\n",
    "def tokenization(text):\n",
    "    text = re.split('\\W+', text)\n",
    "    return text\n",
    "\n",
    "def stem_words(text):\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    word_tokens = tokenization(text)\n",
    "    stem_list = list(map(stemmer.stem, word_tokens))\n",
    "    return \" \".join(stem_list)\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV} \n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    return \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UzOuoOECcc6j"
   },
   "source": [
    "### ALL PREPROCCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "_Xxwdk6RY8MV"
   },
   "outputs": [],
   "source": [
    "pos = preprocessing('data/processedPositive.csv')\n",
    "neg = preprocessing('data/processedNegative.csv')\n",
    "neu = preprocessing('data/processedNeutral.csv')\n",
    "\n",
    "\n",
    "df = concatinate_all_tweets(pos, neg, neu)\n",
    "df['tweet'] = df['tweet'].apply(convert_emoticons_to_word)\n",
    "df = to_lower(df)\n",
    "df = remove_punctuation(df)\n",
    "\n",
    "df[\"stop\"] = df[\"tweet\"].apply(delete_stopwords)\n",
    "df = remove_nan_dupl(df, \"stop\")\n",
    "df[\"common\"] = df[\"stop\"]\n",
    "\n",
    "'''\n",
    "Processing required by subject:\n",
    "'''\n",
    "\n",
    "df[\"token\"] = df['common'].apply(lambda x: tokenization(x.lower()))\n",
    "df[\"token\"] = df['token'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "df[\"stem\"] = df[\"common\"].apply(stem_words)\n",
    "\n",
    "df[\"lemma\"] = df[\"common\"].apply(lemmatize_words)\n",
    "\n",
    "df['misspellings'] = df['common'].apply(lambda x: str(TextBlob(x).correct()))\n",
    "\n",
    "df['stem+misspellings'] = df['misspellings'].apply(stem_words)\n",
    "\n",
    "df['lemmatize+misspellings'] = df['misspellings'].apply(lemmatize_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_O7qrhRdjY4"
   },
   "source": [
    "## Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLKpHwEMY_OT",
    "outputId": "ca29f1f9-1d90-4c7d-d098-1de5c64fcc6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (2118, 6) (2118,)\n",
      "Train data: (530, 6) (530,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = df.copy()\n",
    "\n",
    "y = data['sense']\n",
    "\n",
    "X = data[[\"token\", \"stem\", \"lemma\", \"misspellings\", \"stem+misspellings\", \"lemmatize+misspellings\"]]\n",
    "X_train_all, X_test_all, y_train, y_test = train_test_split(X, y, test_size = 0.2, stratify = y)\n",
    "\n",
    "X_all = pd.concat([X_train_all, X_test_all], axis = 1)\n",
    "\n",
    "print(\"Train data:\", X_train_all.shape, y_train.shape)\n",
    "print(\"Train data:\", X_test_all.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ3eWKxmbsOE"
   },
   "source": [
    "## Transform to vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "JCb8aiZzbytQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def bow(X_train, X_test):\n",
    "    vect = CountVectorizer(binary=True)\n",
    "    X_train_bow = vect.fit_transform(X_train)\n",
    "    X_test_bow = vect.transform(X_test)\n",
    "    X_bow = vstack([X_train_bow, X_test_bow])\n",
    "    return X_train_bow, X_test_bow, X_bow\n",
    "\n",
    "def word_count(X_train, X_test):\n",
    "    vect = CountVectorizer() \n",
    "    X_train_count = vect.fit_transform(X_train)\n",
    "    X_test_count = vect.transform(X_test)\n",
    "    X_count = vstack([X_train_count, X_test_count])\n",
    "    return X_train_count, X_test_count, X_count\n",
    "\n",
    "def tfidf(X_train, X_test):\n",
    "    vect = TfidfVectorizer()\n",
    "    X_train_tfidf = vect.fit_transform(X_train)\n",
    "    X_test_tfidf = vect.transform(X_test)\n",
    "    X_tfidf = vstack([X_train_tfidf, X_test_tfidf])\n",
    "    return X_train_tfidf, X_test_tfidf, X_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HO0b9vbsiS59"
   },
   "outputs": [],
   "source": [
    "vectorized_dict = {}\n",
    "\n",
    "for process in X.columns:\n",
    "    vectorized_key = \"BOW_\" + process\n",
    "    vectorized_dict[vectorized_key] = list(bow(X_train_all[process], X_test_all[process]))\n",
    "\n",
    "    vectorized_key = \"WORDCOUNT_\" + process\n",
    "    vectorized_dict[vectorized_key] = list(word_count(X_train_all[process], X_test_all[process]))\n",
    "\n",
    "    vectorized_key = \"TFIDF_\" + process\n",
    "    vectorized_dict[vectorized_key] = list(tfidf(X_train_all[process], X_test_all[process]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIY_B6w4llwN"
   },
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YWEcTxS2ltUy",
    "outputId": "d45bfe19-8960-4b8b-b8ad-504f7a6f0e80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t BOW_TOKEN \t\t\n",
      "number =  1\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  2\n",
      "waited unhappy\n",
      "waited u unhappy\n",
      "number =  3\n",
      "hi ashish tried call number got response unhappy please share another suitable time alternate cont\n",
      "hi tried call number got response unhappy please share another suitable time alternate number cont\n",
      "number =  4\n",
      "hi tried call number got response unhappy please share another suitable time alternate number us cont\n",
      "hi tried call number got response unhappy please share another suitable time alternate number cont\n",
      "number =  5\n",
      "hey thanks top new followers week much appreciated happy want\n",
      "hey thanks top new followers week much appreciated happy\n",
      "number =  6\n",
      "thanks top engaged community members week happy want\n",
      "thanks top engaged community members week happy want free\n",
      "number =  7\n",
      "share love thanks top new followers week happy\n",
      "share love thanks top new followers week happy want\n",
      "number =  8\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  9\n",
      "share love high value members week happy\n",
      "share love high value members week happy insight\n",
      "number =  10\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "\t\t WORDCOUNT_TOKEN \t\t\n",
      "number =  1\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  2\n",
      "waited unhappy\n",
      "waited u unhappy\n",
      "number =  3\n",
      "hi tried call number got response unhappy please share another suitable time alternate number us cont\n",
      "hi tried call number got response unhappy please share another suitable time alternate number cont\n",
      "number =  4\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "hey thanks top new followers week much appreciated happy want\n",
      "hey thanks top new followers week much appreciated happy\n",
      "number =  8\n",
      "thanks top engaged community members week happy want\n",
      "thanks top engaged community members week happy want free\n",
      "number =  9\n",
      "share love thanks top new followers week happy\n",
      "share love thanks top new followers week happy want\n",
      "number =  10\n",
      "hi ashish tried call number got response unhappy please share another suitable time alternate cont\n",
      "hi tried call number got response unhappy please share another suitable time alternate number cont\n",
      "\t\t TFIDF_TOKEN \t\t\n",
      "number =  1\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  2\n",
      "waited unhappy\n",
      "waited u unhappy\n",
      "number =  3\n",
      "hi tried call number got response unhappy please share another suitable time alternate number us cont\n",
      "hi tried call number got response unhappy please share another suitable time alternate number cont\n",
      "number =  4\n",
      "hey thanks top new followers week much appreciated happy want\n",
      "hey thanks top new followers week much appreciated happy\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "share love thanks top new followers week happy\n",
      "share love thanks top new followers week happy want\n",
      "number =  8\n",
      "thanks recent follow much appreciated happy\n",
      "thanks recent follow much appreciated happy get\n",
      "number =  9\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  10\n",
      "thanks top engaged community members week happy want\n",
      "thanks top engaged community members week happy want free\n",
      "\t\t BOW_STEM \t\t\n",
      "number =  1\n",
      "thank share happi\n",
      "thank share happi\n",
      "number =  2\n",
      "thank b happi\n",
      "thank happi\n",
      "number =  3\n",
      "thank happi\n",
      "thank b happi\n",
      "number =  4\n",
      "thank happi\n",
      "thank happi\n",
      "number =  5\n",
      "wait unhappi\n",
      "wait u unhappi\n",
      "number =  6\n",
      "hi ashish tri call number got respons unhappi pleas share anoth suitabl time altern cont\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number cont\n",
      "number =  7\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number us cont\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number cont\n",
      "number =  8\n",
      "hey thank top new follow week much appreci happi want\n",
      "hey thank top new follow week much appreci happi\n",
      "number =  9\n",
      "thank top engag communiti member week happi want\n",
      "thank top engag communiti member week happi want free\n",
      "number =  10\n",
      "share love thank top new follow week happi\n",
      "share love thank top new follow week happi want\n",
      "\t\t WORDCOUNT_STEM \t\t\n",
      "number =  1\n",
      "thank share happi\n",
      "thank share happi\n",
      "number =  2\n",
      "thank b happi\n",
      "thank happi\n",
      "number =  3\n",
      "thank happi\n",
      "thank b happi\n",
      "number =  4\n",
      "thank happi\n",
      "thank happi\n",
      "number =  5\n",
      "wait unhappi\n",
      "wait u unhappi\n",
      "number =  6\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number us cont\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number cont\n",
      "number =  7\n",
      "thank recent follow happi connect happi great thursday want\n",
      "thank recent follow happi connect happi great thursday want free\n",
      "number =  8\n",
      "thank recent follow happi connect happi great wednesday\n",
      "thank recent follow happi connect happi great wednesday want\n",
      "number =  9\n",
      "thank recent follow happi connect happi great thursday want\n",
      "thank recent follow happi connect happi great thursday\n",
      "number =  10\n",
      "hey thank top new follow week much appreci happi want\n",
      "hey thank top new follow week much appreci happi\n",
      "\t\t TFIDF_STEM \t\t\n",
      "number =  1\n",
      "thank b happi\n",
      "thank happi\n",
      "number =  2\n",
      "thank happi\n",
      "thank b happi\n",
      "number =  3\n",
      "thank happi\n",
      "thank happi\n",
      "number =  4\n",
      "wait unhappi\n",
      "wait u unhappi\n",
      "number =  5\n",
      "thank share happi\n",
      "thank share happi\n",
      "number =  6\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number us cont\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number cont\n",
      "number =  7\n",
      "thank recent follow happi connect happi great wednesday\n",
      "thank recent follow happi connect happi great wednesday want\n",
      "number =  8\n",
      "hey thank top new follow week much appreci happi want\n",
      "hey thank top new follow week much appreci happi\n",
      "number =  9\n",
      "thank recent follow happi connect happi great thursday want\n",
      "thank recent follow happi connect happi great thursday\n",
      "number =  10\n",
      "share love thank top new follow week happi\n",
      "share love thank top new follow week happi want\n",
      "\t\t BOW_LEMMA \t\t\n",
      "number =  1\n",
      "hi try call number get response unhappy please share another suitable time alternate number u cont\n",
      "hi try call number get response unhappy please share another suitable time alternate number cont\n",
      "number =  2\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  3\n",
      "wait unhappy\n",
      "wait u unhappy\n",
      "number =  4\n",
      "hi ashish try call number get response unhappy please share another suitable time alternate cont\n",
      "hi try call number get response unhappy please share another suitable time alternate number cont\n",
      "number =  5\n",
      "hi try call number get response unhappy please share another suitable time alternate number u cont\n",
      "hi ashish try call number get response unhappy please share another suitable time alternate cont\n",
      "number =  6\n",
      "hey thanks top new follower week much appreciate happy want\n",
      "hey thanks top new follower week much appreciate happy\n",
      "number =  7\n",
      "thanks top engage community member week happy want\n",
      "thanks top engage community member week happy want free\n",
      "number =  8\n",
      "share love thanks top new follower week happy\n",
      "share love thanks top new follower week happy want\n",
      "number =  9\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  10\n",
      "share love high value member week happy\n",
      "share love high value member week happy insight\n",
      "\t\t WORDCOUNT_LEMMA \t\t\n",
      "number =  1\n",
      "hi try call number get response unhappy please share another suitable time alternate number u cont\n",
      "hi try call number get response unhappy please share another suitable time alternate number cont\n",
      "number =  2\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  3\n",
      "wait unhappy\n",
      "wait u unhappy\n",
      "number =  4\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "hey thanks top new follower week much appreciate happy want\n",
      "hey thanks top new follower week much appreciate happy\n",
      "number =  8\n",
      "thanks top engage community member week happy want\n",
      "thanks top engage community member week happy want free\n",
      "number =  9\n",
      "share love thanks top new follower week happy\n",
      "share love thanks top new follower week happy want\n",
      "number =  10\n",
      "hi ashish try call number get response unhappy please share another suitable time alternate cont\n",
      "hi try call number get response unhappy please share another suitable time alternate number cont\n",
      "\t\t TFIDF_LEMMA \t\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number =  1\n",
      "hi try call number get response unhappy please share another suitable time alternate number u cont\n",
      "hi try call number get response unhappy please share another suitable time alternate number cont\n",
      "number =  2\n",
      "wait unhappy\n",
      "wait u unhappy\n",
      "number =  3\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  4\n",
      "hey thanks top new follower week much appreciate happy want\n",
      "hey thanks top new follower week much appreciate happy\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "share love thanks top new follower week happy\n",
      "share love thanks top new follower week happy want\n",
      "number =  8\n",
      "thanks recent follow much appreciate happy\n",
      "thanks recent follow much appreciate happy get\n",
      "number =  9\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "thanks recent follow happy connect happy great thursday get free\n",
      "number =  10\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "\t\t BOW_MISSPELLINGS \t\t\n",
      "number =  1\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  2\n",
      "waited unhappy\n",
      "waited u unhappy\n",
      "number =  3\n",
      "hi assist tried call number got response unhappy please share another suitable time alternate count\n",
      "hi tried call number got response unhappy please share another suitable time alternate number count\n",
      "number =  4\n",
      "hi tried call number got response unhappy please share another suitable time alternate number us count\n",
      "hi tried call number got response unhappy please share another suitable time alternate number count\n",
      "number =  5\n",
      "hey thanks top new followers week much appreciated happy want\n",
      "hey thanks top new followers week much appreciated happy\n",
      "number =  6\n",
      "thanks top engaged community members week happy want\n",
      "thanks top engaged community members week happy want free\n",
      "number =  7\n",
      "share love thanks top new followers week happy\n",
      "share love thanks top new followers week happy want\n",
      "number =  8\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  9\n",
      "share love high value members week happy\n",
      "share love high value members week happy insight\n",
      "number =  10\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "\t\t WORDCOUNT_MISSPELLINGS \t\t\n",
      "number =  1\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  2\n",
      "waited unhappy\n",
      "waited u unhappy\n",
      "number =  3\n",
      "hi tried call number got response unhappy please share another suitable time alternate number us count\n",
      "hi tried call number got response unhappy please share another suitable time alternate number count\n",
      "number =  4\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "hey thanks top new followers week much appreciated happy want\n",
      "hey thanks top new followers week much appreciated happy\n",
      "number =  8\n",
      "thanks top engaged community members week happy want\n",
      "thanks top engaged community members week happy want free\n",
      "number =  9\n",
      "share love thanks top new followers week happy\n",
      "share love thanks top new followers week happy want\n",
      "number =  10\n",
      "hi assist tried call number got response unhappy please share another suitable time alternate count\n",
      "hi tried call number got response unhappy please share another suitable time alternate number count\n",
      "\t\t TFIDF_MISSPELLINGS \t\t\n",
      "number =  1\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  2\n",
      "waited unhappy\n",
      "waited u unhappy\n",
      "number =  3\n",
      "hi tried call number got response unhappy please share another suitable time alternate number us count\n",
      "hi tried call number got response unhappy please share another suitable time alternate number count\n",
      "number =  4\n",
      "hey thanks top new followers week much appreciated happy want\n",
      "hey thanks top new followers week much appreciated happy\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "share love thanks top new followers week happy\n",
      "share love thanks top new followers week happy want\n",
      "number =  8\n",
      "thanks recent follow much appreciated happy\n",
      "thanks recent follow much appreciated happy get\n",
      "number =  9\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  10\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "thanks recent follow happy connect happy great thursday get free\n",
      "\t\t BOW_STEM+MISSPELLINGS \t\t\n",
      "number =  1\n",
      "thank share happi\n",
      "thank share happi\n",
      "number =  2\n",
      "thank b happi\n",
      "thank happi\n",
      "number =  3\n",
      "thank happi\n",
      "thank b happi\n",
      "number =  4\n",
      "thank happi\n",
      "thank happi\n",
      "number =  5\n",
      "wait unhappi\n",
      "wait u unhappi\n",
      "number =  6\n",
      "hi assist tri call number got respons unhappi pleas share anoth suitabl time altern count\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number count\n",
      "number =  7\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number us count\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number count\n",
      "number =  8\n",
      "hey thank top new follow week much appreci happi want\n",
      "hey thank top new follow week much appreci happi\n",
      "number =  9\n",
      "thank top engag communiti member week happi want\n",
      "thank top engag communiti member week happi want free\n",
      "number =  10\n",
      "share love thank top new follow week happi\n",
      "share love thank top new follow week happi want\n",
      "\t\t WORDCOUNT_STEM+MISSPELLINGS \t\t\n",
      "number =  1\n",
      "thank share happi\n",
      "thank share happi\n",
      "number =  2\n",
      "thank b happi\n",
      "thank happi\n",
      "number =  3\n",
      "thank happi\n",
      "thank b happi\n",
      "number =  4\n",
      "thank happi\n",
      "thank happi\n",
      "number =  5\n",
      "wait unhappi\n",
      "wait u unhappi\n",
      "number =  6\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number us count\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number count\n",
      "number =  7\n",
      "thank recent follow happi connect happi great thursday want\n",
      "thank recent follow happi connect happi great thursday want free\n",
      "number =  8\n",
      "thank recent follow happi connect happi great wednesday\n",
      "thank recent follow happi connect happi great wednesday want\n",
      "number =  9\n",
      "thank recent follow happi connect happi great thursday want\n",
      "thank recent follow happi connect happi great thursday\n",
      "number =  10\n",
      "hey thank top new follow week much appreci happi want\n",
      "hey thank top new follow week much appreci happi\n",
      "\t\t TFIDF_STEM+MISSPELLINGS \t\t\n",
      "number =  1\n",
      "thank share happi\n",
      "thank share happi\n",
      "number =  2\n",
      "thank b happi\n",
      "thank happi\n",
      "number =  3\n",
      "thank happi\n",
      "thank b happi\n",
      "number =  4\n",
      "thank happi\n",
      "thank happi\n",
      "number =  5\n",
      "wait unhappi\n",
      "wait u unhappi\n",
      "number =  6\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number us count\n",
      "hi tri call number got respons unhappi pleas share anoth suitabl time altern number count\n",
      "number =  7\n",
      "thank recent follow happi connect happi great wednesday\n",
      "thank recent follow happi connect happi great wednesday want\n",
      "number =  8\n",
      "hey thank top new follow week much appreci happi want\n",
      "hey thank top new follow week much appreci happi\n",
      "number =  9\n",
      "thank recent follow happi connect happi great thursday want\n",
      "thank recent follow happi connect happi great thursday\n",
      "number =  10\n",
      "share love thank top new follow week happi\n",
      "share love thank top new follow week happi want\n",
      "\t\t BOW_LEMMATIZE+MISSPELLINGS \t\t\n",
      "number =  1\n",
      "hi try call number get response unhappy please share another suitable time alternate number u count\n",
      "hi try call number get response unhappy please share another suitable time alternate number count\n",
      "number =  2\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  3\n",
      "wait unhappy\n",
      "wait u unhappy\n",
      "number =  4\n",
      "hi assist try call number get response unhappy please share another suitable time alternate count\n",
      "hi try call number get response unhappy please share another suitable time alternate number count\n",
      "number =  5\n",
      "hi try call number get response unhappy please share another suitable time alternate number u count\n",
      "hi assist try call number get response unhappy please share another suitable time alternate count\n",
      "number =  6\n",
      "hey thanks top new follower week much appreciate happy want\n",
      "hey thanks top new follower week much appreciate happy\n",
      "number =  7\n",
      "thanks top engage community member week happy want\n",
      "thanks top engage community member week happy want free\n",
      "number =  8\n",
      "share love thanks top new follower week happy\n",
      "share love thanks top new follower week happy want\n",
      "number =  9\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  10\n",
      "share love high value member week happy\n",
      "share love high value member week happy insight\n",
      "\t\t WORDCOUNT_LEMMATIZE+MISSPELLINGS \t\t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number =  1\n",
      "hi try call number get response unhappy please share another suitable time alternate number u count\n",
      "hi try call number get response unhappy please share another suitable time alternate number count\n",
      "number =  2\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  3\n",
      "wait unhappy\n",
      "wait u unhappy\n",
      "number =  4\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "hey thanks top new follower week much appreciate happy want\n",
      "hey thanks top new follower week much appreciate happy\n",
      "number =  8\n",
      "thanks top engage community member week happy want\n",
      "thanks top engage community member week happy want free\n",
      "number =  9\n",
      "share love thanks top new follower week happy\n",
      "share love thanks top new follower week happy want\n",
      "number =  10\n",
      "hi assist try call number get response unhappy please share another suitable time alternate count\n",
      "hi try call number get response unhappy please share another suitable time alternate number count\n",
      "\t\t TFIDF_LEMMATIZE+MISSPELLINGS \t\t\n",
      "number =  1\n",
      "hi try call number get response unhappy please share another suitable time alternate number u count\n",
      "hi try call number get response unhappy please share another suitable time alternate number count\n",
      "number =  2\n",
      "thanks happy\n",
      "thanks b happy\n",
      "number =  3\n",
      "wait unhappy\n",
      "wait u unhappy\n",
      "number =  4\n",
      "hey thanks top new follower week much appreciate happy want\n",
      "hey thanks top new follower week much appreciate happy\n",
      "number =  5\n",
      "thanks recent follow happy connect happy great wednesday\n",
      "thanks recent follow happy connect happy great wednesday want\n",
      "number =  6\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday\n",
      "number =  7\n",
      "share love thanks top new follower week happy\n",
      "share love thanks top new follower week happy want\n",
      "number =  8\n",
      "thanks recent follow much appreciate happy\n",
      "thanks recent follow much appreciate happy get\n",
      "number =  9\n",
      "thanks recent follow happy connect happy great thursday want free\n",
      "thanks recent follow happy connect happy great thursday get free\n",
      "number =  10\n",
      "thanks recent follow happy connect happy great thursday want\n",
      "thanks recent follow happy connect happy great thursday want free\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import sparse\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse import tril\n",
    "\n",
    "for k, v in vectorized_dict.items():\n",
    "    print('\\t\\t', k.upper(), '\\t\\t')\n",
    "    dataset = v[0]\n",
    "    similarities_sparse = cosine_similarity(dataset, dense_output=False)\n",
    "    processing_name = k.split('_')[1]\n",
    "    coo = coo_matrix(similarities_sparse)\n",
    "    coo.setdiag(0)\n",
    "    coo = tril(coo, k = 1)\n",
    "    coo.eliminate_zeros()\n",
    "    tuples = list(zip(coo.row, coo.col, coo.data))\n",
    "    top_10_numb = sorted(tuples, key = lambda x: x[2], reverse=True)[:10]\n",
    "    for tweets_pair, numb in zip(top_10_numb, range(len(top_10_numb))):\n",
    "        print(\"number = \", numb + 1)\n",
    "        print(X_train_all[processing_name].iloc[tweets_pair[0]])\n",
    "        print(X_train_all[processing_name].iloc[tweets_pair[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lcebb1xfdc42"
   },
   "source": [
    "## Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EL3F-G6Bb2mr",
    "outputId": "50fa35d0-6e7d-41ab-ea57-705b0e80a3e3"
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing the best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization type and preprocessing: BOW_token\n",
      "Model: LR,  Test Accuracy: 0.9150943396226415\n",
      "Model: NB,  Test Accuracy: 0.8943396226415095\n",
      "Model: DT,  Test Accuracy: 0.9\n",
      "Model: RF,  Test Accuracy: 0.9245283018867925\n",
      "\n",
      "Vectorization type and preprocessing: WORDCOUNT_token\n",
      "Model: LR,  Test Accuracy: 0.9169811320754717\n",
      "Model: NB,  Test Accuracy: 0.8981132075471698\n",
      "Model: DT,  Test Accuracy: 0.9018867924528302\n",
      "Model: RF,  Test Accuracy: 0.9283018867924528\n",
      "\n",
      "Vectorization type and preprocessing: TFIDF_token\n",
      "Model: LR,  Test Accuracy: 0.9245283018867925\n",
      "Model: NB,  Test Accuracy: 0.8924528301886793\n",
      "Model: DT,  Test Accuracy: 0.9056603773584906\n",
      "Model: RF,  Test Accuracy: 0.9207547169811321\n",
      "\n",
      "Vectorization type and preprocessing: BOW_stem\n",
      "Model: LR,  Test Accuracy: 0.9169811320754717\n",
      "Model: NB,  Test Accuracy: 0.9075471698113208\n",
      "Model: DT,  Test Accuracy: 0.9075471698113208\n",
      "Model: RF,  Test Accuracy: 0.9226415094339623\n",
      "\n",
      "Vectorization type and preprocessing: WORDCOUNT_stem\n",
      "Model: LR,  Test Accuracy: 0.9207547169811321\n",
      "Model: NB,  Test Accuracy: 0.9075471698113208\n",
      "Model: DT,  Test Accuracy: 0.9\n",
      "Model: RF,  Test Accuracy: 0.9226415094339623\n",
      "\n",
      "Vectorization type and preprocessing: TFIDF_stem\n",
      "Model: LR,  Test Accuracy: 0.9264150943396227\n",
      "Model: NB,  Test Accuracy: 0.8924528301886793\n",
      "Model: DT,  Test Accuracy: 0.8943396226415095\n",
      "Model: RF,  Test Accuracy: 0.9245283018867925\n",
      "\n",
      "Vectorization type and preprocessing: BOW_lemma\n",
      "Model: LR,  Test Accuracy: 0.9169811320754717\n",
      "Model: NB,  Test Accuracy: 0.9075471698113208\n",
      "Model: DT,  Test Accuracy: 0.9132075471698113\n",
      "Model: RF,  Test Accuracy: 0.9283018867924528\n",
      "\n",
      "Vectorization type and preprocessing: WORDCOUNT_lemma\n",
      "Model: LR,  Test Accuracy: 0.9150943396226415\n",
      "Model: NB,  Test Accuracy: 0.9056603773584906\n",
      "Model: DT,  Test Accuracy: 0.9056603773584906\n",
      "Model: RF,  Test Accuracy: 0.9245283018867925\n",
      "\n",
      "Vectorization type and preprocessing: TFIDF_lemma\n",
      "Model: LR,  Test Accuracy: 0.9169811320754717\n",
      "Model: NB,  Test Accuracy: 0.8943396226415095\n",
      "Model: DT,  Test Accuracy: 0.8981132075471698\n",
      "Model: RF,  Test Accuracy: 0.9264150943396227\n",
      "\n",
      "Vectorization type and preprocessing: BOW_misspellings\n",
      "Model: LR,  Test Accuracy: 0.9207547169811321\n",
      "Model: NB,  Test Accuracy: 0.8962264150943396\n",
      "Model: DT,  Test Accuracy: 0.9018867924528302\n",
      "Model: RF,  Test Accuracy: 0.9188679245283019\n",
      "\n",
      "Vectorization type and preprocessing: WORDCOUNT_misspellings\n",
      "Model: LR,  Test Accuracy: 0.9207547169811321\n",
      "Model: NB,  Test Accuracy: 0.8962264150943396\n",
      "Model: DT,  Test Accuracy: 0.9018867924528302\n",
      "Model: RF,  Test Accuracy: 0.9226415094339623\n",
      "\n",
      "Vectorization type and preprocessing: TFIDF_misspellings\n",
      "Model: LR,  Test Accuracy: 0.9264150943396227\n",
      "Model: NB,  Test Accuracy: 0.8962264150943396\n",
      "Model: DT,  Test Accuracy: 0.9056603773584906\n",
      "Model: RF,  Test Accuracy: 0.9207547169811321\n",
      "\n",
      "Vectorization type and preprocessing: BOW_stem+misspellings\n",
      "Model: LR,  Test Accuracy: 0.9245283018867925\n",
      "Model: NB,  Test Accuracy: 0.9056603773584906\n",
      "Model: DT,  Test Accuracy: 0.9\n",
      "Model: RF,  Test Accuracy: 0.9320754716981132\n",
      "\n",
      "Vectorization type and preprocessing: WORDCOUNT_stem+misspellings\n",
      "Model: LR,  Test Accuracy: 0.9264150943396227\n",
      "Model: NB,  Test Accuracy: 0.9056603773584906\n",
      "Model: DT,  Test Accuracy: 0.9018867924528302\n",
      "Model: RF,  Test Accuracy: 0.930188679245283\n",
      "\n",
      "Vectorization type and preprocessing: TFIDF_stem+misspellings\n",
      "Model: LR,  Test Accuracy: 0.9283018867924528\n",
      "Model: NB,  Test Accuracy: 0.8849056603773585\n",
      "Model: DT,  Test Accuracy: 0.8886792452830189\n",
      "Model: RF,  Test Accuracy: 0.9283018867924528\n",
      "\n",
      "Vectorization type and preprocessing: BOW_lemmatize+misspellings\n",
      "Model: LR,  Test Accuracy: 0.9207547169811321\n",
      "Model: NB,  Test Accuracy: 0.9113207547169812\n",
      "Model: DT,  Test Accuracy: 0.8924528301886793\n",
      "Model: RF,  Test Accuracy: 0.9188679245283019\n",
      "\n",
      "Vectorization type and preprocessing: WORDCOUNT_lemmatize+misspellings\n",
      "Model: LR,  Test Accuracy: 0.9188679245283019\n",
      "Model: NB,  Test Accuracy: 0.9113207547169812\n",
      "Model: DT,  Test Accuracy: 0.8924528301886793\n",
      "Model: RF,  Test Accuracy: 0.9245283018867925\n",
      "\n",
      "Vectorization type and preprocessing: TFIDF_lemmatize+misspellings\n",
      "Model: LR,  Test Accuracy: 0.9264150943396227\n",
      "Model: NB,  Test Accuracy: 0.8886792452830189\n",
      "Model: DT,  Test Accuracy: 0.8905660377358491\n",
      "Model: RF,  Test Accuracy: 0.9245283018867925\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [ \n",
    "    ('LR', LogisticRegression()), \n",
    "    ('NB', MultinomialNB()), \n",
    "    ('DT', DecisionTreeClassifier()),\n",
    "    ('RF', RandomForestClassifier())]\n",
    "\n",
    "for i in vectorized_dict.keys():\n",
    "    print(f'Vectorization type and preprocessing: {i}') \n",
    "    X_train = vectorized_dict[i][0]\n",
    "    X_test = vectorized_dict[i][1]\n",
    "\n",
    "    for name, model in models:\n",
    "        clf = model\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(f'Model: {name}, ', end=' ')\n",
    "        print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_token\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9216310471251005\n",
      "WORDCOUNT_token\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9206876508986854\n",
      "TFIDF_token\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9117142090673344\n",
      "BOW_stem\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9206809442904408\n",
      "WORDCOUNT_stem\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9187986229097737\n",
      "TFIDF_stem\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9112425109541269\n",
      "BOW_lemma\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9216310471251005\n",
      "WORDCOUNT_lemma\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9192725565590628\n",
      "TFIDF_lemma\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9112425109541269\n",
      "BOW_misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'liblinear'}\n",
      "accuracy : 0.919277027631226\n",
      "WORDCOUNT_misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'liblinear'}\n",
      "accuracy : 0.9192725565590628\n",
      "TFIDF_misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9121859071805419\n",
      "BOW_stem+misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'liblinear'}\n",
      "accuracy : 0.9192725565590628\n",
      "WORDCOUNT_stem+misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9183313958687294\n",
      "TFIDF_stem+misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.913129303406957\n",
      "BOW_lemmatize+misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9197397836001071\n",
      "WORDCOUNT_lemmatize+misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9192725565590628\n",
      "TFIDF_lemmatize+misspellings\n",
      "tuned hyperparameters : (best parameters)  {'max_iter': 100, 'solver': 'lbfgs'}\n",
      "accuracy : 0.9112425109541269\n"
     ]
    }
   ],
   "source": [
    "for i in vectorized_dict.keys():\n",
    "    print(i)\n",
    "    X_train = vectorized_dict[i][0]\n",
    "    X_test = vectorized_dict[i][1]\n",
    "    \n",
    "    grid = [{'solver' : ['lbfgs','newton-cg','liblinear'],\n",
    "            'max_iter' : [100, 1000, 2500, 5000] \n",
    "            }]\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "    logreg_cv = GridSearchCV(logreg, grid, cv=10)\n",
    "    logreg_cv.fit(X_train, y_train)\n",
    "\n",
    "    print(\"tuned hyperparameters : (best parameters) \", logreg_cv.best_params_)\n",
    "    print(\"accuracy :\", logreg_cv.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_final_logreg = []\n",
    "for i in vectorized_dict.keys():\n",
    "\n",
    "    X_train = vectorized_dict[i][0]\n",
    "    X_test = vectorized_dict[i][1]\n",
    "\n",
    "    clf = LogisticRegression(solver = 'lbfgs', max_iter = 100)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_final_logreg.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 or 1, if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just tokenization</th>\n",
       "      <td>0.915094</td>\n",
       "      <td>0.916981</td>\n",
       "      <td>0.924528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stemming</th>\n",
       "      <td>0.916981</td>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.926415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatization</th>\n",
       "      <td>0.916981</td>\n",
       "      <td>0.915094</td>\n",
       "      <td>0.916981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Misspellings</th>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.926415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stemming+misspellings</th>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.926415</td>\n",
       "      <td>0.928302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatization+misspellings</th>\n",
       "      <td>0.920755</td>\n",
       "      <td>0.918868</td>\n",
       "      <td>0.926415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0 or 1, if the word exists  word counts     TFIDF\n",
       "processing                                                                   \n",
       "just tokenization                             0.915094     0.916981  0.924528\n",
       "Stemming                                      0.916981     0.920755  0.926415\n",
       "Lemmatization                                 0.916981     0.915094  0.916981\n",
       "Misspellings                                  0.920755     0.920755  0.926415\n",
       "Stemming+misspellings                         0.924528     0.926415  0.928302\n",
       "Lemmatization+misspellings                    0.920755     0.918868  0.926415"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing = ['just tokenization', \n",
    "                'Stemming', \n",
    "                'Lemmatization', \n",
    "                'Misspellings', \n",
    "                'Stemming+misspellings', \n",
    "                'Lemmatization+misspellings']\n",
    "\n",
    "accuracy_final_logreg = np.array(accuracy_final_logreg).reshape([6, 3])\n",
    "accuracy_final_logreg = pd.DataFrame(accuracy_final_logreg, columns=['0 or 1, if the word exists', 'word counts', 'TFIDF'])\n",
    "accuracy_final_logreg.insert(0, 'processing', processing, allow_duplicates = False)\n",
    "accuracy_final_logreg.set_index('processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid search RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOW_token\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9192725565590628\n",
      "\n",
      "WORDCOUNT_token\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9178574622194402\n",
      "\n",
      "TFIDF_token\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9192747920951444\n",
      "\n",
      "BOW_stem\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 50}\n",
      "accuracy : 0.9140816417776982\n",
      "\n",
      "WORDCOUNT_stem\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9131315389430386\n",
      "\n",
      "TFIDF_stem\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9154922650451576\n",
      "\n",
      "BOW_lemma\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9140816417776982\n",
      "\n",
      "WORDCOUNT_lemma\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 50}\n",
      "accuracy : 0.9136121792005722\n",
      "\n",
      "TFIDF_lemma\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9188053295180183\n",
      "\n",
      "BOW_misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9145488688187428\n",
      "\n",
      "WORDCOUNT_misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9169140659930249\n",
      "\n",
      "TFIDF_misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9202271304658856\n",
      "\n",
      "BOW_stem+misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9112447464902083\n",
      "\n",
      "WORDCOUNT_stem+misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 50}\n",
      "accuracy : 0.9159639631583654\n",
      "\n",
      "TFIDF_stem+misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9173857641062325\n",
      "\n",
      "BOW_lemmatize+misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 50}\n",
      "accuracy : 0.9088795493159261\n",
      "\n",
      "WORDCOUNT_lemmatize+misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 50}\n",
      "accuracy : 0.9084056156666369\n",
      "\n",
      "TFIDF_lemmatize+misspellings\n",
      "tuned hyperparameters :(best parameters)  {'n_estimators': 100}\n",
      "accuracy : 0.9159706697666099\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in vectorized_dict.keys():\n",
    "    print(i)\n",
    "    X_train = vectorized_dict[i][0]\n",
    "    X_test = vectorized_dict[i][1]\n",
    "    \n",
    "    grid = {'n_estimators': [5, 10, 50, 100]}\n",
    "    \n",
    "    model = RandomForestClassifier(random_state=21)\n",
    "    grid_search = GridSearchCV(model, grid, cv=10)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    print(\"tuned hyperparameters :(best parameters) \", grid_search.best_params_)\n",
    "    print(\"accuracy :\", grid_search.best_score_)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RandomForest best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_final = []\n",
    "\n",
    "for i in vectorized_dict.keys():\n",
    "\n",
    "    X_train = vectorized_dict[i][0]\n",
    "    X_test = vectorized_dict[i][1]\n",
    "\n",
    "    clf = RandomForestClassifier(n_estimators = 100)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    score = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    accuracy_final.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0 or 1, if the word exists</th>\n",
       "      <th>word counts</th>\n",
       "      <th>TFIDF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>processing</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>just tokenization</th>\n",
       "      <td>0.922642</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.920755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stemming</th>\n",
       "      <td>0.930189</td>\n",
       "      <td>0.922642</td>\n",
       "      <td>0.920755</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatization</th>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.924528</td>\n",
       "      <td>0.924528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Misspellings</th>\n",
       "      <td>0.913208</td>\n",
       "      <td>0.922642</td>\n",
       "      <td>0.926415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Stemming+misspellings</th>\n",
       "      <td>0.928302</td>\n",
       "      <td>0.932075</td>\n",
       "      <td>0.922642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lemmatization+misspellings</th>\n",
       "      <td>0.916981</td>\n",
       "      <td>0.922642</td>\n",
       "      <td>0.924528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0 or 1, if the word exists  word counts     TFIDF\n",
       "processing                                                                   \n",
       "just tokenization                             0.922642     0.924528  0.920755\n",
       "Stemming                                      0.930189     0.922642  0.920755\n",
       "Lemmatization                                 0.924528     0.924528  0.924528\n",
       "Misspellings                                  0.913208     0.922642  0.926415\n",
       "Stemming+misspellings                         0.928302     0.932075  0.922642\n",
       "Lemmatization+misspellings                    0.916981     0.922642  0.924528"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processing = ['just tokenization', \n",
    "                'Stemming', \n",
    "                'Lemmatization', \n",
    "                'Misspellings', \n",
    "                'Stemming+misspellings', \n",
    "                'Lemmatization+misspellings']\n",
    "\n",
    "result = np.array(accuracy_final).reshape([6, 3])\n",
    "result = pd.DataFrame(result, columns=['0 or 1, if the word exists', 'word counts', 'TFIDF'])\n",
    "result.insert(0, 'processing', processing, allow_duplicates = False)\n",
    "result.set_index('processing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similar to happy_face_or_smiley:\n",
      "[('give', 0.9925644993782043), ('get', 0.9920628666877747), ('make', 0.9917730689048767), ('ill', 0.9912694096565247), ('people', 0.9909273386001587), ('week', 0.9906089305877686), ('sad', 0.9903519749641418), ('top', 0.9902238249778748), ('read', 0.9899414777755737), ('election', 0.9897988438606262)]\n",
      "\n",
      "Shapes of Train and Test w2v datasets:\n",
      "(2118, 20) (530, 20)\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "size = 20\n",
    "window = 3\n",
    "min_count = 15\n",
    "workers = 3\n",
    "sg = 1\n",
    "batch_words = 50\n",
    "X_train_tokens = X_train_all['lemma'].apply(tokenization)\n",
    "X_test_tokens = X_test_all['lemma'].apply(tokenization)\n",
    "X_tokens = X['lemma'].apply(tokenization)\n",
    "tokens = X_tokens.values\n",
    "w2v_model = Word2Vec(tokens, batch_words = batch_words, min_count = min_count, vector_size = size, workers = workers, window = window, sg = sg)\n",
    "print(\"Similar to happy_face_or_smiley:\")\n",
    "print(w2v_model.wv.most_similar('happy_face_or_smiley'))\n",
    "\n",
    "def mean_vectors(tokens):\n",
    "    vec = np.zeros(size)\n",
    "    count = 0\n",
    "    for word in tokens:\n",
    "        try:\n",
    "            vec += w2v_model.wv[word]\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "train_word2vec_matrix = np.zeros(([len(X_train_tokens), size]))\n",
    "for i in range(len(X_train_tokens)):\n",
    "    train_word2vec_matrix[i, :] = mean_vectors(X_train_tokens.iloc[i])\n",
    "X_train_word2vec = pd.DataFrame(train_word2vec_matrix)\n",
    "\n",
    "test_word2vec_matrix = np.zeros(([len(X_test_tokens), size]))\n",
    "for i in range(len(X_test_tokens)):\n",
    "    test_word2vec_matrix[i, :] = mean_vectors(X_test_tokens.iloc[i])\n",
    "X_test_word2vec = pd.DataFrame(test_word2vec_matrix)\n",
    "print(\"\\nShapes of Train and Test w2v datasets:\")\n",
    "print(X_train_word2vec.shape, X_test_word2vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.809433962264151\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train_word2vec, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test_word2vec)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
